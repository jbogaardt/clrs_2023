{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Vt3v7rdLn7u",
        "tags": []
      },
      "outputs": [],
      "source": [
        "!pip install chainladder pycaret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CowEx9QoMohs",
        "tags": []
      },
      "source": [
        "# WS-3 - Introduction to Machine Learning\n",
        "**CLRS 2023**<br>\n",
        "Monday, September 11, 2023 8:00 AM – 11:30 AM East Coast USA Time<br>\n",
        "Location: Hamilton/Indian River<br>\n",
        "CE: 3.6<br>\n",
        "Non-Recorded Session\n",
        "\n",
        "Workshop Speaker(s)\n",
        "* John Bogaardt, FCAS, MAAA - Chief Actuary WCF Insurance\n",
        "* Lahiru Somaratne, MSc in Actuarial Science\n",
        "\n",
        "This 3.5 hour workshop will provide an introduction to machine learning for Actuaries in Python. The workshop is hands-on and will explore the fitting and tuning of several traditional machine learning models using the scikit-learn API. We will briefly expand on the scikit-learn workflow as a modeling framework for reserving through the chainladder-python library. Finally, we will kick it up a notch and explore deep learning examples with Tensorflow/Keras. Emphasis will be on tools and workflow more than a deep dive in theory. The workshop will only briefly cover Python basics, so participants with beginner Python knowledge or fluency in another programming language would have an advantage. We will be using Google Colab for hands-on instruction and participants will need a google account to access Colab. Participants may choose to set up their own environment but no support will be given by the instructors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5dC2KgSB0opQ"
      },
      "source": [
        "## Basic Python Syntax - Crash Course"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "9gztadeV0opR"
      },
      "outputs": [],
      "source": [
        "1 + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "xNJWc6kW0opS"
      },
      "outputs": [],
      "source": [
        "x = 5\n",
        "y = 2\n",
        "z = x**y\n",
        "z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "rGCLqWos0opT"
      },
      "outputs": [],
      "source": [
        "if x > y:\n",
        "    print('x is greater than y')\n",
        "elif x == y:\n",
        "    print('x is equal to y')\n",
        "else:\n",
        "    print('x is less than y')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQgd8ECa0opU"
      },
      "source": [
        "A list is what you would expect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "qT93LWX_0opU"
      },
      "outputs": [],
      "source": [
        "my_list = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "my_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a9J6QTd0opU"
      },
      "source": [
        "But lists can be sliced..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "IV2r2y_70opV"
      },
      "outputs": [],
      "source": [
        "my_list[1:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "69ayOS0R0opV"
      },
      "outputs": [],
      "source": [
        "my_list[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "56hWjLqI0opW"
      },
      "outputs": [],
      "source": [
        "my_list[-3:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "KfIbK37V0opW"
      },
      "outputs": [],
      "source": [
        "my_list[::-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wms2yPq-0opX"
      },
      "source": [
        "We can loop over list elements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "4VnfcZEf0opX"
      },
      "outputs": [],
      "source": [
        "accumulate = 0\n",
        "for item in my_list:\n",
        "    accumulate = accumulate + item\n",
        "print(accumulate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ohoT0zf0opY"
      },
      "source": [
        "A \"comprehension\" is a for loop and a list put together in one line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "gai9IFCv0opY"
      },
      "outputs": [],
      "source": [
        "[2*item for item in my_list]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJtShwYi0opY"
      },
      "source": [
        "Dictionaries are like lists with labels. They are key/value pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "3qSe5vIM0opY"
      },
      "outputs": [],
      "source": [
        "my_dict = {'a': 1, 'b': 2, 'c': 3, 'd': 4, 'e': 5}\n",
        "my_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8h7OAEK0opZ"
      },
      "source": [
        "You can access any `value` of a dictionary by its `key`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "BKPOsswX0opZ"
      },
      "outputs": [],
      "source": [
        "my_dict['b']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gco234hE0opZ"
      },
      "source": [
        "However dictionaries make no guarantees about their order, so slicing is not permitted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "T9lz0b7H0opZ"
      },
      "outputs": [],
      "source": [
        "my_dict[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uda9Zqe0opZ"
      },
      "source": [
        "But you can loop over all key/value pairs in a dictionary with a \"comprehension\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "eSAIvI5L0opa"
      },
      "outputs": [],
      "source": [
        "{key: value**2 for key, value in my_dict.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzFavOez0opa"
      },
      "source": [
        "A function is a stateless piece of code. Given some inputs it creates an output. These are useful for encapsulating logic into a shortened name. Functions are like verbs. They do things."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "MDf6IFI30opa"
      },
      "outputs": [],
      "source": [
        "def exponent(x=1, y=0):\n",
        "    return x**y\n",
        "\n",
        "exponent(5, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trABob2s0opa"
      },
      "source": [
        "A class is a blueprint for a stateful object. An object can have properties (attributes), and they can also do things (methods). Methods are just a fancy name for functions that are tied to a class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "NK3LIfAD0opa"
      },
      "outputs": [],
      "source": [
        "class Exponent:\n",
        "    def __init__(self, x=1, y=0):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def exponent(self):\n",
        "        return self.x ** self.y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZKi3knB0opb"
      },
      "source": [
        "With a class defined, we can create instances or objects of that class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "sMLiIzxV0opb"
      },
      "outputs": [],
      "source": [
        "instance = Exponent(5, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q8Bqvxu0opb"
      },
      "source": [
        "...and get access to all the pre-defined attributes and methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "QblJ9EiI0opb"
      },
      "outputs": [],
      "source": [
        "print(instance.x, instance.y)\n",
        "instance.exponent()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csDzpaKw0opb"
      },
      "source": [
        "**Everything is an object in python!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "YrWnRw1O0opc"
      },
      "source": [
        "## The Scientific Python Stack\n",
        "<img src=\"https://jupytearth.org/_images/python-stack.png\" width=\"1000\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqUs12Hp0opc"
      },
      "source": [
        "## Importing Packages\n",
        "\n",
        "The python standard library is useful for basic scripting of computer tasks, but it is not that great for data and analysis.\n",
        "\n",
        "All of these packages have their own **functions** and **classes** stored in one or more **modules**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "QTuytz9T0opc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import chainladder as cl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "%config InlineBackend.figure_format = 'retina'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-scOb2-DMohw"
      },
      "source": [
        "# The Basics of Machine Learning with Scikit-Learn\n",
        "`sklearn` is the defacto standard Machine Learning  API in python.\n",
        "\n",
        "How do you get the most out of your data with Machine Learning with `sklearn`? An opinionated view:\n",
        "\n",
        "| Technique | Return on Investment |\n",
        "|-|-|\n",
        "|Feature Engineering|⭐⭐⭐⭐⭐|\n",
        "|Model selection|⭐⭐⭐⭐|\n",
        "|Hyperparameter tuning|⭐⭐⭐|\n",
        "|Model Ensembling|⭐⭐⭐|\n",
        "|Feature Preprocessing|⭐⭐|\n",
        "\n",
        "\n",
        "Let's explore the API."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BG0WP5GfMohw",
        "tags": []
      },
      "source": [
        "## Estimators\n",
        "\n",
        "Almost everything in `sklearn` is an estimator.  Estimators have properties that are worth remembering.\n",
        "\n",
        "Estimators are python objects that carry out a Machine Learning task. These objects can do things (`fit`, `predict`) and have properties associated with them.\n",
        "\n",
        "Depending on the algorithm you wish to use, you will need to import the specific `sklearn` class from which you'll create your estimator. This will typically follow the following structure:\n",
        "\n",
        "```python\n",
        "from some_module import Estimator\n",
        "estimator = Estimator()\n",
        "```\n",
        "Estimators are instances of a class (which is denoted by Uppercase naming convention). Creating an instance of a model is as simple as adding parenthesis at the end of the class name.\n",
        "\n",
        "Let's import the LinearRegression class and create an instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "649lsLyzMohy",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOaU1uiKMoh0",
        "tags": []
      },
      "source": [
        "👉 _All estimator objects can optionally be configured with **hyper**parameters to uniquely specify the model being built. This is done ahead of exposing any data to the model._\n",
        "\n",
        "Hyperparameters are parameters that are set before the learning process of a machine learning algorithm. Unlike the model's parameters, which are learned from the data during training, hyperparameters are predefined and determined by the user or data scientist. These hyperparameters influence the behavior and performance of the learning algorithm.\n",
        "\n",
        "```python\n",
        "estimator = Estimator(param1=1, param2=2)\n",
        "```\n",
        "\n",
        "`sklearn` has sensible default hyperparameters. These can be viewed and optionally overridden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuGYhhNfMoh1",
        "tags": []
      },
      "outputs": [],
      "source": [
        "model.get_params()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDJSeY3bMoh2"
      },
      "source": [
        "Note that we can set our **hyperparameters** in the absence of looking at any data. These parameters are there to tell the model how it should behave during the fitting process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGQW1O73Moh2",
        "tags": []
      },
      "outputs": [],
      "source": [
        "model = LinearRegression(fit_intercept=False)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9mLJQYIJMoh3"
      },
      "source": [
        "👉 _All estimator objects expose a `fit` method that takes as input, `X`_:\n",
        "\n",
        "```python\n",
        "estimator.fit(X, y)\n",
        "```\n",
        "Models also optionally support a response, `y` or and weights, `sample_weight`.\n",
        "\n",
        "\n",
        "## Synthetic data\n",
        "Let's create some _independent_ variables, **X** to explore the concept."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "DoVjt2jM0oph"
      },
      "outputs": [],
      "source": [
        "group = np.random.rand(200)\n",
        "group = np.where(group<.4, 'a', np.where(group<.8, 'b', 'c'))\n",
        "age = np.linspace(16, 100, 200)\n",
        "X = pd.DataFrame({'group': group, 'age': age})\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "ENoOSxZ10oph"
      },
      "source": [
        "Let's also syntehsize a _dependent_ variable, **y** that depends on the values of **X**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q45w6OyFMoh3",
        "tags": []
      },
      "outputs": [],
      "source": [
        "y = 30 + X['group'].map({'a': 1.5, 'b': 1.0, 'c': 0.5}) * X['age'] + np.random.normal(0, 15, 200)\n",
        "y[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48rra_CZMoh4"
      },
      "source": [
        "We know the linear equation that generated this data, but because we added an error term, we can see the _noise_ in our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ban-otojMoh4",
        "tags": []
      },
      "outputs": [],
      "source": [
        "pd.concat((X, pd.Series(y, name='y')), axis=1).plot(\n",
        "    kind='scatter',\n",
        "    x='age', y='y',\n",
        "    title='Simple Regression Data',\n",
        "    c=X['group'].map({'a': 'blue', 'b': 'green', 'c': 'orange'}));"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LFZNCmZuMoh5"
      },
      "source": [
        "## The `fit` method\n",
        "Let's fit our model to our data just using `age`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-oV9ixUMoh5",
        "tags": []
      },
      "outputs": [],
      "source": [
        "model = LinearRegression().fit(X[['age']], y)\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k51-pUEEMoh5"
      },
      "source": [
        "It looks like nothing happened, but in actuality, our model has be fitted to the data.  The `model` object itself displays the same, but it now has additional features.  \n",
        "\n",
        "👉 _When a model is fit, the fitted attributes are denoted by a trailing **underscore**._\n",
        "\n",
        "```python\n",
        "estimator.estimated_param_\n",
        "```\n",
        "\n",
        "The use of the underscore is a key API design style of scikit-learn that allows for the quicker recognition of fitted parameters vs hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ByzCbO5GMoh6",
        "tags": []
      },
      "outputs": [],
      "source": [
        "f'The slope of the model is {model.coef_} and the intercept is {model.intercept_}.'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQXes2nqMoh6"
      },
      "source": [
        "## The `score` method\n",
        "Can we do statistical inference with `sklearn`?\n",
        "\n",
        "Only in limited ways at an overall level. There are no coefficient-specific t-statistics, p-values, etc, associated with this model retained by `sklearn`. The machine learning focus of the library promotes overall model performance over and will have metrics for model accuracy, R-squared, AUC, etc.  \n",
        "\n",
        "👉 _Scoring can only be invoked after a model has been `fit`._\n",
        "\n",
        "By default regression problems use _R2_ as a scoring metric and classification uses _accuracy_. These can be changed and alternatives are found in the `sklearn.metrics` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snC1Yuc0Moh6",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print('Model R2:',  model.score(X[['age']], y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "4twIPnkq0opo"
      },
      "source": [
        "## The `predict` method\n",
        "\n",
        "We can also make predictions with a fitted model using the `predict` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "51eG0Co8Moh7",
        "tags": []
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(X[['age']])\n",
        "predictions[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7K6phvD4Moh8",
        "tags": []
      },
      "outputs": [],
      "source": [
        "ax = pd.concat((X, pd.Series(y, name='y')), axis=1).plot(\n",
        "    kind='scatter', x='age', y='y',\n",
        "    title='Linear Approximation to Data');\n",
        "\n",
        "pd.concat(\n",
        "    (X[['age']], pd.Series(predictions, name='y')),\n",
        "    axis=1\n",
        ").plot(\n",
        "    kind='line', x='age', y='y',\n",
        "    ax=ax, legend=False,\n",
        "    linestyle='dashed');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "oSc2t-Tv0opp"
      },
      "source": [
        "Fitting the entire independent variableset including our `group` feature doesn't work. Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "QqWScxWK0opp"
      },
      "outputs": [],
      "source": [
        "model.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2_SBJZKMoh8"
      },
      "source": [
        "## Transformers (Preprocessing)\n",
        "\n",
        "`sklearn` comes with a a toolbox of preprocessing **Transformers** that can help specific algorithms react more appropriately to your data. Many algorithms have limitations. For example they may:\n",
        " * Can only deal with numeric values (OneHotEncoder or OrdinalEncoder)\n",
        " * Hate missing values (SimpleImputer or KNNImputer)\n",
        " * Use Euclidean distance for decision boundaries (StandardScaler)\n",
        " * Don't deal well with non-linear data (KBinsDiscretizer)\n",
        " * Don't explicitely recognize interactions (PolynomialFeatures)\n",
        "\n",
        "\n",
        "A **Transformer** is just like the Estimators we learned about above with a couple of exceptions:\n",
        "1. They transform our feature-space **X**\n",
        "2. They have a `transform` method instead of a `predict` and `score` method.\n",
        "\n",
        "We can see that the `linear_model` doesn't like categorical data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "RdG51i8a0opq"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "ohe = OneHotEncoder()\n",
        "ohe.fit(X[['group']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gvgicdL70opr"
      },
      "source": [
        "Once fit, transformers have fitted attributes (denoted by trailing underscore)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "TDJ13H2B0opr"
      },
      "outputs": [],
      "source": [
        "ohe.categories_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "r-dKFaS60opr"
      },
      "source": [
        "We can then use our transform our featureset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "SCOt89PG0opr"
      },
      "outputs": [],
      "source": [
        "print(X['group'].head())\n",
        "ohe.transform(X[['group']]).todense()[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9Jwu7-p0ops"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "nHUok3Wa0ops"
      },
      "outputs": [],
      "source": [
        "X_transformed = pd.concat((\n",
        "    pd.DataFrame(\n",
        "        ohe.transform(X[['group']]).todense(),\n",
        "        columns=ohe.categories_[0]),\n",
        "    X['age']),\n",
        "    axis=1)\n",
        "\n",
        "X_transformed.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "x00a7BwY0ops"
      },
      "source": [
        "We can fit our model to this featureset, and see we get better performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "K_EkgFNI0ops"
      },
      "outputs": [],
      "source": [
        "model.fit(X_transformed, y)\n",
        "model.score(X_transformed, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "QXfFgBd_0opt"
      },
      "source": [
        "Still, the form is not correct. This assumes each group and age are independent features. However, the source equation is an interaction between age and group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "yZ4ZV1KJ0opt"
      },
      "outputs": [],
      "source": [
        "pd.Series(model.coef_, index=model.feature_names_in_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "-8pSvMy70opt"
      },
      "source": [
        "Let's also create the interaction terms we know are in the model (because we synthesized this data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "LXwifwe80opu"
      },
      "outputs": [],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "qNTOw7Ah0opv"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "pf = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
        "pf.fit(X_transformed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "hW5v1VG-0opv"
      },
      "outputs": [],
      "source": [
        "X_transformed = pd.DataFrame(\n",
        "    pf.transform(X_transformed),\n",
        "    columns=pf.get_feature_names_out())\n",
        "X_transformed.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "TnFExNB-0opw"
      },
      "source": [
        "We can now fit our model assuming the age:group interactions are the only relevant features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "clhv_9BX0opw"
      },
      "outputs": [],
      "source": [
        "model = LinearRegression().fit(X_transformed[['a age', 'b age', 'c age']], y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "jf4muBSF0opx"
      },
      "outputs": [],
      "source": [
        "print('Compared to true coefficients of [1.5, 1.0, 0.5], our model coefficients are', model.coef_)\n",
        "print('Compared to true intercept of 30, our model coefficients are', model.intercept_)\n",
        "print('Model R2:', model.score(X_transformed[['a age', 'b age', 'c age']], y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "KHD4ekgR0opx"
      },
      "source": [
        "😩 These transformers are tedious. When new data comes in, the same transformations have to be applied to make a prediction.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jny6a7950opx"
      },
      "source": [
        "## Pipelines (Composite Estimators)\n",
        "\n",
        "Piecing transformers and predictors together is a very common pattern and it is nice to do so succintly. With `Pipeline`s, one can create custom end-to-end models.\n",
        "\n",
        "We do so by specifying a list of steps that should be executed in order.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "lJrRVx1K0opy"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('group_transform', ColumnTransformer([('ohe', OneHotEncoder(), ['group']), ('pass', 'passthrough', ['age'])])),\n",
        "    ('create_interaction', PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)),\n",
        "    ('retain_only_interactions', ColumnTransformer([('keep', 'passthrough', [6, 8, 9])])),\n",
        "    ('model', LinearRegression())\n",
        "])\n",
        "pipe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "e8JjnzMD0opy"
      },
      "source": [
        "Setting up a `Pipeline` can be complex, but once done, it makes everything else easier, like fitting:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "W4P-Ut4K0opy"
      },
      "outputs": [],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "i87NOvqa0opz"
      },
      "outputs": [],
      "source": [
        "pipe.fit(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4B_Gsy5W0opz"
      },
      "source": [
        "scoring..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "eEYybnxF0opz"
      },
      "outputs": [],
      "source": [
        "pipe.score(X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zz-sUWbd0opz"
      },
      "source": [
        "or making new predictions on raw data..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "cl6q665n0opz"
      },
      "outputs": [],
      "source": [
        "print(X.iloc[10:11])\n",
        "\n",
        "print('prediction', pipe.predict(X.iloc[10:11]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "7E0rho0W0op0"
      },
      "source": [
        "Most importantly, `Pipeline`s encapsulate data transformations.\n",
        "\n",
        "👉 _`Pipeline`s substantially minimize the risk of data leakage and are crucial for managing model generalization when preprocessing data is involved._\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6syQ2GX0op0"
      },
      "source": [
        "Let's look at the Regression Pipeline we created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "wrY5rTxH0op0"
      },
      "outputs": [],
      "source": [
        "ax = pd.concat((X, pd.Series(y, name='y')), axis=1).plot(\n",
        "    kind='scatter',\n",
        "    x='age', y='y',\n",
        "    title='Simple Regression Data',\n",
        "    c=X['group'].map({'a': 'blue', 'b': 'green', 'c': 'orange'}));\n",
        "\n",
        "pd.DataFrame({'age': X[X['group']=='a']['age'], 'y': pipe.predict(X[X['group']=='a'])}).plot(\n",
        "    kind='line', x='age', y='y', ax=ax, legend=False, linestyle='dashed', color='blue');\n",
        "pd.DataFrame({'age': X[X['group']=='b']['age'], 'y': pipe.predict(X[X['group']=='b'])}).plot(\n",
        "    kind='line', x='age', y='y', ax=ax, legend=False, linestyle='dashed', color='green');\n",
        "pd.DataFrame({'age': X[X['group']=='c']['age'], 'y': pipe.predict(X[X['group']=='c'])}).plot(\n",
        "    kind='line', x='age', y='y', ax=ax, legend=False, linestyle='dashed', color='orange');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "zI_CHNO30op0"
      },
      "source": [
        "# A Classification Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAvTKFjXMoh8"
      },
      "source": [
        "These are the very basics of `sklearn`, but let's try something on little more interesting with reserving triangles.\n",
        "\n",
        "`sklearn` has estimators for clustering, classification, and regression.\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://scikit-learn.org/stable/_static/ml_map.png\" width=\"1000\"/>\n",
        "\n",
        "\n",
        "Diving deep into all areas is beyond the scope of this seminar. We will instead explore the scope of ML through a classification use-case.\n",
        "\n",
        "**Can we determine the line of business of a loss triangle from the triangle itself?**\n",
        "\n",
        "\n",
        "## The Dataset\n",
        "### A `chainladder` Triangle\n",
        "\n",
        "Let's explore the `clrd`  ([CAS Loss Reserve Database](https://www.casact.org/publications-research/research/research-resources/loss-reserving-data-pulled-naic-schedule-p)) dataset stored within the `chainladder` python package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KjFqRlfmogZy",
        "tags": []
      },
      "outputs": [],
      "source": [
        "clrd = cl.load_sample('clrd')\n",
        "clrd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OI1ZxZa8Moh9",
        "tags": []
      },
      "source": [
        "We are using a `chainladder.Triangle` to explore the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EHlSUh2_Moh9",
        "tags": []
      },
      "outputs": [],
      "source": [
        "type(clrd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm-90f8LMoh-"
      },
      "source": [
        "The `Triangle` object is like a pandas DataFrame with an individual triangle as a datatype.  We can have different rows of a triangle representing Companies and Lines of Business.  We can have different rows covering the different measures like Paid Loss, Premium, etc.  The result is a 4D object (Table of Triangles):\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/casact/chainladder-python/master/docs/images/triangle_graphic.PNG\" width=\"1000\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "9CWriWb20op1"
      },
      "source": [
        "Our `clrd` triangle expressed as a DataFrame abstraction:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "QO5rLIRw0op1"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(\n",
        "    index=clrd.index,\n",
        "    columns=list(clrd.columns),\n",
        "    data=np.repeat(np.repeat([[str(clrd.shape[2:])]], clrd.shape[0], axis=0), clrd.shape[1], axis=1)).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inEih1FWMoh-",
        "tags": []
      },
      "source": [
        "### Similarity to Pandas\n",
        "The `Triangle` object is very similar to a pandas DataFrame and uses a subset of the pandas API for data manipulation. There are several advantages in having a dedicated Triangle object:\n",
        "\n",
        "* Actuaries work with sets of triangles. DataFrames, being two dimensional, support single triangles with grace but become unwieldy with multiple triangles.\n",
        "* We can carry through the meaningful pandas functionality while also supporting triangle specific methods not found in pandas\n",
        "* Improved memory footprint with sparse array representation in backend\n",
        "* Calculated fields with “virtual” columns allows for lazy column evaluation of Triangles as well as improved memory footprint for larger triangles.\n",
        "\n",
        "Ultimately, there are a lot of things pandas can do that are not relevant to reserving, and there are a lot of things a Triangle needs to do that are not handled easily with pandas.  That said, a lot of pandas functionality works with Triangles.  For example, we can slice individual cells out of our table of Triangles by expressing a row and column combination. When looking at a single cell, we get the detailed representatin of that cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "cy6x_pHN0op2"
      },
      "outputs": [],
      "source": [
        "clrd.loc[('Agway Ins Co', 'comauto')]['CumPaidLoss']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMqD3cQS0op2"
      },
      "source": [
        "We can calculate aggregate values over all \"rows\" of the `Triangle`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5wxmwu3Moh-",
        "tags": []
      },
      "outputs": [],
      "source": [
        "clrd['CumPaidLoss'].sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGqoAFu7AenQ"
      },
      "source": [
        "We can filter the triangle to a specific LOB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuH9gQtAMoh-",
        "tags": []
      },
      "outputs": [],
      "source": [
        "clrd[clrd['LOB']=='ppauto']['CumPaidLoss'].sum().T.plot(xlabel='Age', title='Personal Auto Paid Loss');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JJu63OQMoh_"
      },
      "source": [
        "We can also do things that pandas doesn't support such as explore the age-to-age factors or `link_ratio` of any triangle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPrG67FeMoh_",
        "tags": []
      },
      "outputs": [],
      "source": [
        "clrd[clrd['LOB']=='ppauto']['CumPaidLoss'].sum().link_ratio.heatmap()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZD0_dsDfMoh_",
        "tags": []
      },
      "source": [
        "Creating a new column in a Triangle looks identical to the same operation in pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmSFb3lQMoh_",
        "tags": []
      },
      "outputs": [],
      "source": [
        "clrd['CaseIncurLoss'] = clrd['IncurLoss'] - clrd['BulkLoss']\n",
        "clrd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PjX8CA5a0op3"
      },
      "source": [
        "### Feature Engineering\n",
        "\n",
        "Feature engineering is the process of selecting, transforming, and creating new features from raw data to improve the performance of machine learning models. It is a crucial step in a machine learning as it can have a significant impact on the accuracy and effectiveness of the models.\n",
        "\n",
        "The importance of feature engineering cannot be understated.  In deep learning frameworks, automatic feature engineering is one of the key reasons why it has become so powerful and successful in various machine learning tasks. Traditional machine learning algorithms often rely on handcrafted features, where domain experts manually engineer relevant features from raw data. However, this process can be time-consuming, error-prone, and may not capture all the underlying patterns in complex datasets.\n",
        "\n",
        "`sklearn` is a traditional machine learning framework, so we are going to rely on our reserving expertise (and `chainladder`) to engineer features. As actuaries, we know that some lines of business are short-tailed and some are long-tailed. This information is encoded in the loss development patterns of a triangle. It makes sense that these patterns might also prove useful for a machine learning algorithm trying to predict line of business."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNQTBNt-MoiJ"
      },
      "source": [
        "#### Link Ratios\n",
        "`chainladder` has estimators that follow the sklearn convention too. We can define a model and fit it to some data.\n",
        "\n",
        "We can define a model by creating an instance of a `chainladder` transformer. Here we take a simple average of link-ratios for each development age of a Triangle.\n",
        "\n",
        "Here `cl.Development` is a `sklearn`-compliant estimator and `average` is a _hyperparameter_."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wIx1XDxGMoiJ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "dev_model = cl.Development(average='simple')\n",
        "dev_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZcMTJXd9MoiJ"
      },
      "source": [
        "Reserving is more like an unsupervised technique. We don't typically have a response variable, `y`.  Ignoring `y` is expected in `chainladder`, and for `sklearn` unsupervised techniques."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GciaXM3lMoiJ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "dev_model.fit(X=clrd['CumPaidLoss'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4-BnAeVB3Yn"
      },
      "source": [
        "Like sklearn, fitting our estimator exposes fitted parameters. The `Development` estimator has a `ldf_` parameter representing the selected age-to-age factors which are also represented by the `Triangle` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFiKmSsWMoiK",
        "tags": []
      },
      "outputs": [],
      "source": [
        "paid_link_ratios = dev_model.ldf_\n",
        "paid_cdfs = dev_model.cdf_\n",
        "\n",
        "paid_link_ratios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxrKoG39CaG3"
      },
      "source": [
        "Like pandas we can slice into our Triangle to see a subset of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHTevwTqMoiK",
        "tags": []
      },
      "outputs": [],
      "source": [
        "paid_link_ratios.loc['Allstate Ins Co Grp'].loc['wkcomp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grOfOWv7E3rb"
      },
      "source": [
        "Let's derive the `ldf_` patterns for `CaseIncurLoss`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xmy7tPamMoiK",
        "tags": []
      },
      "outputs": [],
      "source": [
        "inc_link_ratios = dev_model.fit(X=clrd['CaseIncurLoss']).ldf_\n",
        "inc_cdfs = dev_model.cdf_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8F2TuFNMoiK",
        "tags": []
      },
      "outputs": [],
      "source": [
        "inc_link_ratios.loc['Allstate Ins Co Grp'].loc['wkcomp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNOEJRqW0op5"
      },
      "source": [
        "#### Paid to Incurred Ratio\n",
        "The ratio of paid amounts to incurred amounts gives information on how fast claims are reported vs settled. A line like Workers' Compensation has fast reporting but slow settlement. Medical Malpractice might have slow reporting and slow settlement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ZrFVIn3R0op5"
      },
      "outputs": [],
      "source": [
        "clrd['Paid-to-Incurred'] = clrd['CumPaidLoss'] / clrd['CaseIncurLoss']\n",
        "paid_to_inc = clrd['Paid-to-Incurred'].mean(axis='origin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "gHPc2hQn0op5"
      },
      "outputs": [],
      "source": [
        "paid_to_inc.loc['Allstate Ins Co Grp'].loc['wkcomp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "nT9S9wSq0op5"
      },
      "source": [
        "#### Incremental Paid on Prior Case\n",
        "Another variation on the paid to incurred is the incremental paid on prior case reserve. It measures what percentage of the prior year-end case reserves are paid out in the subsequent year."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "4-n8RU-10op5"
      },
      "outputs": [],
      "source": [
        "clrd['CaseReserves'] = clrd['CaseIncurLoss'] - clrd['CumPaidLoss']\n",
        "\n",
        "increm_paid_per_case = (\n",
        "    clrd['CumPaidLoss'].cum_to_incr().iloc[..., 1:] /\n",
        "    clrd['CaseReserves'].iloc[..., :-1].values\n",
        ").mean('origin')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "EmRZGGat0op6"
      },
      "outputs": [],
      "source": [
        "increm_paid_per_case.loc['Allstate Ins Co Grp'].loc['wkcomp']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNKccdloMoiM"
      },
      "source": [
        "### Tidy Data\n",
        "\n",
        "In order to explore this possibility, we need to **tidy** our dataset up.\n",
        "\n",
        "1. Each variable in the dataset should be represented by its own column.\n",
        "2. Each row in the dataset represents a separate observation or case. The dataset should contain individual records or instances, where each row corresponds to a unique observation.\n",
        "3. Each cell in the dataset should hold a single value. This means that there should be no cells that contain multiple values or combine different types of information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2JACEb7I-oM"
      },
      "source": [
        "Our data now has one row per observation (triangle). There are 725 observations with 55 features where ach feature represents a origin/development combination from a 10 x 10 triangle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6Ae2d8FIJJy",
        "tags": []
      },
      "outputs": [],
      "source": [
        "paid_link_ratios = paid_link_ratios.to_frame()\n",
        "paid_link_ratios.columns = [c + ' Paid Link Ratio' for c in paid_link_ratios.columns]\n",
        "\n",
        "inc_link_ratios = inc_link_ratios.to_frame()\n",
        "inc_link_ratios.columns = [c + ' Incurred Link Ratio' for c in inc_link_ratios.columns]\n",
        "\n",
        "paid_to_inc = paid_to_inc.to_frame()\n",
        "paid_to_inc.columns = [str(c) + ' Paid to Incurred' for c in paid_to_inc.columns]\n",
        "\n",
        "increm_paid_per_case = increm_paid_per_case.to_frame()\n",
        "increm_paid_per_case.columns = [str(c) + ' Incremental Paid to Prior Case' for c in increm_paid_per_case.columns]\n",
        "\n",
        "paid_cdfs = paid_cdfs.to_frame()\n",
        "paid_cdfs.columns = [c + ' Paid CDF' for c in paid_cdfs.columns]\n",
        "\n",
        "inc_cdfs = inc_cdfs.to_frame()\n",
        "inc_cdfs.columns = [c + ' Incurred CDF' for c in inc_cdfs.columns]\n",
        "\n",
        "data = pd.concat([paid_link_ratios, inc_link_ratios, paid_to_inc, increm_paid_per_case, paid_cdfs, inc_cdfs], axis=1)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpALvV1y0op6"
      },
      "source": [
        "### Removing sparse outliers\n",
        "The `clrd` dataset has a lot of very sparse triangles that don't have any meaningful information in them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "9OiPQetL0op6"
      },
      "outputs": [],
      "source": [
        "clrd.loc['Adriatic Ins Co', 'CumPaidLoss'].loc['othliab']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHAv1kxF0op6"
      },
      "source": [
        "It is usually a good idea to eliminate noisy or non-informative records from our training data. Bad records typically do not improve model performance and may actually hurt it.\n",
        "\n",
        "Let's define how \"full\" each triangle is by counting the number of Premium entries each has."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "A5Axx78y0op7"
      },
      "outputs": [],
      "source": [
        "prem_density = np.minimum(clrd['EarnedPremDIR'], 1).sum('origin').sum('development').to_frame() / 55"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Qlk7g5Cn0op7"
      },
      "outputs": [],
      "source": [
        "prem_density.loc['Adriatic Ins Co'].loc['othliab']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UI5CblTu0op7"
      },
      "source": [
        "...and optionally remove lightly populated triangles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "IWe-39tz0op7"
      },
      "outputs": [],
      "source": [
        "data = data.loc[prem_density[prem_density>.5].index]\n",
        "\n",
        "print(sum(prem_density<=0.5), 'records have been removed for being sparse.', sum(prem_density>0.5), 'have been retained.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO0mJUMF0op7"
      },
      "source": [
        "## Response encoding\n",
        "As was the case with our featureset, our response variable needs to be numeric for it to work with several ML algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "wc4kwkKT0op7"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "_hYPZE6V0op7"
      },
      "source": [
        "We want to predict `LOB`,which happens to be a categorical value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "IQ7MWCg60op7"
      },
      "outputs": [],
      "source": [
        "data.index.get_level_values('LOB')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "bis2BZ1C0op8"
      },
      "source": [
        "More scikit-learn transformers to the rescure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5MVNM__LBTY",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "le.fit(data.index.get_level_values('LOB'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "dSxS6pjV0op8"
      },
      "outputs": [],
      "source": [
        "y = le.transform(data.index.get_level_values('LOB'))\n",
        "print(le.classes_)\n",
        "y[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "OhNTLBbG0op8"
      },
      "outputs": [],
      "source": [
        "X = data.reset_index(drop=True)\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_PTqJiufJ-6J"
      },
      "source": [
        "## Data Splitting\n",
        "\n",
        "The train-test split is a crucial step in the machine learning process, and it serves the improtant purpose of testing whether our models generalize well. Overfitting occurs when a model performs well on the training data but fails to generalize to new data. By evaluating the model on a separate test set, you can assess whether it has learned to capture meaningful patterns or if it is merely memorizing the training data. This helps in detecting and mitigating overfitting issues.\n",
        "\n",
        "There are several splitting strategies available, but we will use a simple random splitter with 75% in training and 25% in test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IamHD2qTJ37s",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "  X, y, test_size = 0.25, random_state = 42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WJ1DluU0op8"
      },
      "source": [
        "## Our First Model\n",
        "\n",
        "Let's start with the `KNeighborsClassifier`. It works based on the idea that similar data points tend to have similar labels or target values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "jIfIr7hl0op9"
      },
      "outputs": [],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "knn_model = KNeighborsClassifier()\n",
        "knn_model.fit(X_train, y_train)\n",
        "print(f'Model Training Accuracy: {knn_model.score(X_train, y_train)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CxcBQC_0op9"
      },
      "source": [
        "Let's also try a `RandomForestClassifier` which is an ensemble method that combines multiple decision trees to make a prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "z0-MLWDF0op9"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rf_model = RandomForestClassifier()\n",
        "rf_model.fit(X_train, y_train)\n",
        "print(f'Model Training Accuracy: {rf_model.score(X_train, y_train)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nhHRJI20op9"
      },
      "source": [
        "In classification problems, looking beyond accuracy at precision, recall, and F1 score is informative, especially for multinomial classification. These are difficult to remember terms, but are nicely encapsualted in the `confusion_matrix` which is a matrix of actual classification vs predicted classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "-Y-tk3j_0op9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = pd.DataFrame(\n",
        "    confusion_matrix(y_train, rf_model.predict(X_train)),\n",
        "    index=le.classes_,\n",
        "    columns=le.classes_)\n",
        "cm.style.background_gradient(cmap='Blues', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "681ctNlr0op9"
      },
      "source": [
        "Accuracy is just sum of the diagonal of the confusion matrix over all entries in the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "pbp9jI020op-"
      },
      "outputs": [],
      "source": [
        "print(f'Model Training Accuracy: {np.diag(cm).sum() / cm.sum().sum()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "7oM9mnYB0op-"
      },
      "source": [
        "This is amazing accuracy! But we're skeptical that it is representative of how good the model will perform on our test dataset.  That said, we don't want to look at our test dataset because if we use performance on test to refine our model, then we are leaking information from our test set back into training.\n",
        "\n",
        "👉 _Never look at your holdout dataset until you are done with all training iterations of your models._\n",
        "\n",
        "We overcome this issue with cross-validation on our training dataset.\n",
        "\n",
        "## Cross Validation\n",
        "We can use cross-validation to emulate the results expected on a true holdout set.\n",
        "![](https://upload.wikimedia.org/wikipedia/commons/4/4b/KfoldCV.gif)\n",
        "\n",
        "In `sklearn`, this is achieved by passing the estimator and the data into the `cross_val_score` function. `cross_val_score` defaults to 5 80/20 random splits of our training data. It trains on the 80% and tests on the 20%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "LJf7zSAU0op-"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import KFold, cross_validate\n",
        "\n",
        "cv=KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "cv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "QmZTUhU00op-"
      },
      "outputs": [],
      "source": [
        "rf_cv_score = cross_validate(rf_model, X_train, y_train, cv=cv)\n",
        "print(rf_cv_score['test_score'])\n",
        "print('Average Cross-Validated Accuracy:', rf_cv_score['test_score'].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zceZaUL50op_"
      },
      "source": [
        "This is woefully short of the accuracy on the training data. Is our model providing any value at all?  It's certainly better than KNeighbors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "6H7dI_6-0op_"
      },
      "outputs": [],
      "source": [
        "knn_cv_score = cross_validate(KNeighborsClassifier(), X_train, y_train, cv=cv)\n",
        "print(f'KNeighbors Cross-Validated Accuracy:', knn_cv_score['test_score'].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyJIPoKX0op_"
      },
      "source": [
        "## The NULL Model\n",
        "\n",
        "We can randomly guess a classification and (assuming data is evenly distributed) expect 1/6 or 17% accuracy from chance. However, our data is not evenly distributed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "izz4WAo30op_"
      },
      "outputs": [],
      "source": [
        "pd.Series(\n",
        "    le.inverse_transform(y_train)\n",
        ").value_counts().plot(\n",
        "    kind='bar',\n",
        "    title='Frequency of LOBs in Training Dataset');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "lSFl3K7F0op_"
      },
      "source": [
        "We can use `sklearn`s Dummy estimators to find out. Like all other estimators, it conforms to the same API and can be used with cross-validation, pipelines, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "W86aLX6A0op_"
      },
      "outputs": [],
      "source": [
        "from sklearn.dummy import DummyClassifier\n",
        "dummy_model_cv = cross_validate(DummyClassifier(), X_train, y_train, cv=cv)\n",
        "\n",
        "print(\"Dummy Accuracy:\", dummy_model_cv['test_score'].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSMt6hlI0op_"
      },
      "source": [
        "Our RandomForestClassifier is better than the DummyClassifer, so its providing at least some value, but is it the best it can be?\n",
        "\n",
        "## Parameter Tuning\n",
        "`sklearn` has sensible defaults for hyperparameters, but changing these will alter how the model works and ultimately will have consequences for model performance. `GridSearchCV` is a very simple and brute-force approach to testing various hyperparameters.  It is effectively a loop over all hyperparameters you want to test (as specified by `param_grid`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "YpHxrUgi0op_"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "rf_grid = GridSearchCV(\n",
        "    RandomForestClassifier(),\n",
        "    param_grid={\n",
        "        'n_estimators': [50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
        "    },\n",
        "    cv=cv)\n",
        "\n",
        "rf_grid.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "O7gip2GR0oqA"
      },
      "outputs": [],
      "source": [
        "pd.DataFrame(rf_grid.cv_results_).set_index('param_n_estimators')[['mean_test_score']].plot(\n",
        "    title='Average Cross-validated accuracy by Forest Size');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "jXU9LpM80oqA"
      },
      "source": [
        "When tuning more than one hyperparameter, `GridSearchCV` will fit each unique combination of your parameter grid.  Here we have 1x4x3=12 parameter sets, cross-validated 5 times, yields 60 models.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "FQZvv4Bh0oqA"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "rf_grid = GridSearchCV(\n",
        "    RandomForestClassifier(),\n",
        "    param_grid={\n",
        "        'class_weight': ['balanced'],\n",
        "        'n_estimators': [250, 500, 750, 1000],\n",
        "        'max_depth': [10, 20, 30],}, cv=cv)\n",
        "\n",
        "rf_grid.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "_sODhVvp0oqA"
      },
      "outputs": [],
      "source": [
        "pd.pivot_table(\n",
        "    data=pd.DataFrame(rf_grid.cv_results_),\n",
        "    index=['param_n_estimators'],\n",
        "    columns='param_max_depth',\n",
        "    values='mean_test_score'\n",
        ").style.background_gradient(cmap='Blues', axis=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "-ieVzR7F0oqA"
      },
      "outputs": [],
      "source": [
        "tuned_rf_cv_score = cross_validate(rf_grid.best_estimator_, X_train, y_train, cv=cv)\n",
        "print(f'Our Tuned Model accuracy:', tuned_rf_cv_score['test_score'].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "L2hZmenS0oqA"
      },
      "source": [
        "`GridSeachCV` is horribly inefficient. That said, the approach of testing various hyperparameters is fundamentally how all hyperparameter optimization routines work. Other routines will only run combinations that are likely to produce model improvement.  For further reference, see `sklearn` documentation of [Hyperparameter Optimizers](https://scikit-learn.org/stable/modules/classes.html#hyper-parameter-optimizers)\n",
        "or third-party packages such as [Optuna](https://optuna.org/) or [Hyperopt](http://hyperopt.github.io/hyperopt/) for more scalable approaches."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "RgFpHjEC0oqA"
      },
      "source": [
        "## Gradient Boosted Machines (GBM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "MIl8wBVk0oqB"
      },
      "source": [
        "In recent years, gradient boosted machine, specifically libraries like LightGBM and XGBoost, have become increasingly popular in the field of machine learning and data science. They tend to be more robust to many ML painpoint such as:\n",
        "1. imbalanced data (classification)\n",
        "2. computational efficiency\n",
        "3. missing data\n",
        "4. outliers\n",
        "5. Non-linear relationships\n",
        "6. accuracy\n",
        "\n",
        "Much of the `Pipeline` preprocessing work we did in \"The Basics\" with Linear Models become much less important with GBM. As modeling competitions go for tabular data, GBM dominates the rankings.\n",
        "\n",
        "\n",
        "These libraries are also completely compatible with the `sklearn` API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKdurDe2MoiM",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "\n",
        "gbm_cv_score = cross_validate(LGBMClassifier(verbose=-1), X_train, y_train, cv=cv)\n",
        "print(f'Our Tuned Model accuracy:', gbm_cv_score['test_score'].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "mrKhvCLj0oqB"
      },
      "source": [
        "Like `sklearn`-native estimators, they can be tuned as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c_MpGIjxMoiO",
        "tags": []
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "gbm_grid = GridSearchCV(\n",
        "    LGBMClassifier(verbose=-1),\n",
        "    param_grid={\n",
        "        'num_leaves': [2, 5, 10, 15],\n",
        "        'learning_rate': [.01, .1],\n",
        "        'reg_alpha': [.5, 1],\n",
        "        'reg_lambda': [0, .5,]})\n",
        "\n",
        "gbm_grid.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "VAUdq0uI0oqB"
      },
      "outputs": [],
      "source": [
        "pd.pivot_table(\n",
        "    pd.DataFrame(gbm_grid.cv_results_),\n",
        "    index=['param_num_leaves', 'param_learning_rate'],\n",
        "    columns=['param_reg_alpha', 'param_reg_lambda'],\n",
        "    values='mean_test_score', aggfunc='mean'\n",
        ").style.background_gradient(cmap='Blues', axis=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9YYvt5oMoiP",
        "tags": []
      },
      "outputs": [],
      "source": [
        "tuned_gbm_cv_score = cross_validate(gbm_grid.best_estimator_, X_train, y_train, cv=cv)\n",
        "print('LightGBM Tuned Model accuracy:', tuned_gbm_cv_score['test_score'].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbW9M2An0oqB"
      },
      "source": [
        "## Ensembling\n",
        "\n",
        "Another strategy to improve ML performance is to blend multiple algorithms together. Different algorithms have differing strengths and weaknesses.  Combining the effects of different models can help"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ULBFiYQK0oqB"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "sc = StackingClassifier(\n",
        "    estimators=[\n",
        "        ('knn', KNeighborsClassifier()),\n",
        "        ('rf', RandomForestClassifier()),\n",
        "        ('gbm', LGBMClassifier(verbose=-1))])\n",
        "\n",
        "sc.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "18HY5idP0oqC"
      },
      "outputs": [],
      "source": [
        "stacked_cv_score = cross_validate(sc, X_train, y_train, cv=cv)\n",
        "stacked_cv_score['test_score'].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "cQESUyPT0oqC"
      },
      "source": [
        "As with all other Estimators, Ensembles also support hyperparameter optimization. In order to avoid hyperparameter name clashes between different sub-estimators,\n",
        "\n",
        "_👉 `StackingClassifier` and `Pipeline` use a **double underscore** convention to separate the sub-estimator name and hyperparameter name._"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYZ1tmN_0oqC"
      },
      "source": [
        "In this way, we are not optimizing any one model, but the entire ensemble."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "pxeMzsKc0oqC"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "stack_grid = GridSearchCV(\n",
        "    sc,\n",
        "    param_grid={\n",
        "        'knn__n_neighbors': [3, 8],\n",
        "        'rf__max_depth': [10, 30],\n",
        "        'gbm__num_leaves': [5, 15]})\n",
        "\n",
        "stack_grid.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "CBdTBVtG0oqC"
      },
      "outputs": [],
      "source": [
        "tuned_stacked_cv_score = cross_validate(stack_grid.best_estimator_, X_train, y_train, cv=cv)\n",
        "tuned_stacked_cv_score['test_score'].mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3L4KiK00oqC"
      },
      "source": [
        "## Model Generalization\n",
        "\n",
        "At this point, we are seeing diminishing returns with our experiments. We've used `cross_val_score` as a proxy for whether our model will generalize well.  On that basis, the tuned LightGBM model provides the greatest accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "Y8rKv3sJ0oqC"
      },
      "outputs": [],
      "source": [
        "pd.Series({\n",
        "    'Dummy': dummy_model_cv['test_score'].mean(),\n",
        "    'Default KNeighbors': knn_cv_score['test_score'].mean(),\n",
        "    'Default Random Forest': rf_cv_score['test_score'].mean(),\n",
        "    'Tuned Random Forest': tuned_rf_cv_score['test_score'].mean(),\n",
        "    'Default LightGBM': gbm_cv_score['test_score'].mean(),\n",
        "    'Tuned LightGBM': tuned_gbm_cv_score['test_score'].mean(),\n",
        "    'Default Stacking': stacked_cv_score['test_score'].mean(),\n",
        "    'Tuned Stacking': tuned_stacked_cv_score['test_score'].mean()\n",
        "}).sort_values(ascending=False).plot(kind='barh', title='Cross-validated Accuracy Comparison');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ed2QKX9n0oqC"
      },
      "source": [
        "### Performance on Test\n",
        "\n",
        "We should expect our cross-validated performance to be similar to our holdout performance if:\n",
        "1. Train/Test Split data distributions are not biased\n",
        "2. There is no data leakage between Train/Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "_XKwadub0oqD"
      },
      "outputs": [],
      "source": [
        "gbm_grid.best_estimator_.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "R-c_4U7M0oqD"
      },
      "outputs": [],
      "source": [
        "cm = pd.DataFrame(\n",
        "    confusion_matrix(y_test, gbm_grid.best_estimator_.predict(X_test)),\n",
        "    index=le.classes_,\n",
        "    columns=le.classes_)\n",
        "cm.style.background_gradient(cmap='Blues', axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8LwiJwo0oqD"
      },
      "source": [
        "### Know your metric\n",
        "'prodliab' and 'medmal' are disproportionately classed as 'othliab'. This is a very common problem with imbalanced datasets. Underrepresented classes tend to perform poorly when optimizing for _accuracy_. In cases such as fraud detection accuracy would not be the best measure of model success.\n",
        "\n",
        "How does one deal with imbalanced data?\n",
        "- Some of the algorithms we explored `RandomForestClassifier` and `LGBMClassifier` have a `class_weight` hyperparameter that can be set to 'balanced'.\n",
        "- Some algorithms support a `sample_weight` in the `fit` method. `model.fit(X, y, sample_weight=...)` this can also be used to give more weight to certain observations.\n",
        "- Other packages like [imbalanced-learn](https://imbalanced-learn.org/stable/) are `sklearn` compatible packages that add additional functionality to upsample or downsample data to address class imbalance.\n",
        "\n",
        "In all cases, you will likely use another metric (like Precision, Recall, F1, ROC) to gauge which model and hyperparameter set are the msot appropriate.\n",
        "\n",
        "\n",
        "Consider an extremely imbalanced binary classification problem. Our featureset is made of random numbers that have no relationship to our response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "kLbtC2P90oqD"
      },
      "outputs": [],
      "source": [
        "X_imbalanced = np.random.rand(500, 20) # 20 columns of random uniform numbers\n",
        "y_imbalanced = (np.random.rand(500)<.01)*1 # 1% of response variable is 1 99% is 0.\n",
        "\n",
        "print('Dummy Accuracy on highly imbalanced data:',\n",
        "      cross_validate(DummyClassifier(),\n",
        "                     X_imbalanced,\n",
        "                     y_imbalanced,\n",
        "                     cv=cv)['test_score'].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ue4eUwAA0oqD"
      },
      "source": [
        "One simply needs to pass the `scoring` argument to `GridSeachCV` and/or `cross_validate` to chose a different scoring mechanism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "5d-rn9Hw0oqD"
      },
      "outputs": [],
      "source": [
        "rf_grid = GridSearchCV(\n",
        "    RandomForestClassifier(),\n",
        "    param_grid={'n_estimators': [100, 500]},\n",
        "    scoring='f1_weighted', # change the scoring algorithm from accuracy to F1\n",
        "    cv=cv)\n",
        "\n",
        "rf_grid.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "-7N1R9220oqG"
      },
      "outputs": [],
      "source": [
        "print('Default Random Forest F1:', cross_validate(RandomForestClassifier(), X_train, y_train, cv=cv, scoring='f1_weighted')['test_score'].mean())\n",
        "print('Tuned Random Forest F1:', cross_validate(rf_grid.best_estimator_, X_train, y_train, cv=cv, scoring='f1_weighted')['test_score'].mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTXdkYXX0oqG"
      },
      "source": [
        "## Model Explainability\n",
        "\n",
        "Model explainability is crucial for for several reasons. It aids the practitioner in identifying weaknesses in the model. It promotes refinement in feature engineering.  It promotes the discovery of _bias_ and _ethical_ concerns in the model. It's key for communicating what drives the model decisions and gaining stakeholder trust.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Importance\n",
        "\n",
        "Feature importance is built into several algorithms. In particular the tree-based algorithms include this attribute and it shows the relative importance or contribution of each feature in a machine learning model's predictions.\n",
        "\n",
        "For tree-based models, such as Decision Trees and Random Forests, feature importance is usually computed based on how much each feature contributes to reducing impurity or variance across the trees. The more a feature is used to split the data and the more it reduces the impurity, the higher its importance.\n",
        "\n",
        "In GBMs, feature importance is determined by the average improvement in the model's loss function (e.g., mean squared error or log loss) that is achieved when a particular feature is used for splitting during the boosting process."
      ],
      "metadata": {
        "id": "sUqEwswJLcaj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "ekb1IshG0oqH"
      },
      "outputs": [],
      "source": [
        "pd.Series(\n",
        "    rf_grid.best_estimator_.feature_importances_,\n",
        "    index=rf_grid.best_estimator_.feature_names_in_\n",
        ").sort_values(ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "1V0NXdtE0oqH"
      },
      "outputs": [],
      "source": [
        "pd.Series(\n",
        "    gbm_grid.best_estimator_.feature_importances_,\n",
        "    index=gbm_grid.best_estimator_.feature_name_\n",
        ").sort_values(ascending=False).head(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zvxok15M0oqH"
      },
      "source": [
        "### Partial Dependence\n",
        "Partial dependence of a feature provide a global view of the feature's impact, averaging across all instances in the dataset. In multiclassification, these plots show the probability of of the target class along a univariate view."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "wzKLw6JL0oqH"
      },
      "outputs": [],
      "source": [
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "\n",
        "fig, ((ax0, ax1, ax2), (ax3, ax4, ax5)) = plt.subplots(ncols=3, nrows=2, figsize=(15,10))\n",
        "features=[list(gbm_grid.best_estimator_.feature_name_).index('12_Paid_to_Incurred')]\n",
        "\n",
        "PartialDependenceDisplay.from_estimator(gbm_grid.best_estimator_, X_train, features=features, target=[0], ax=ax0)\n",
        "PartialDependenceDisplay.from_estimator(gbm_grid.best_estimator_, X_train, features=features, target=[1], ax=ax1)\n",
        "PartialDependenceDisplay.from_estimator(gbm_grid.best_estimator_, X_train, features=features, target=[2], ax=ax2)\n",
        "PartialDependenceDisplay.from_estimator(gbm_grid.best_estimator_, X_train, features=features, target=[3], ax=ax3)\n",
        "PartialDependenceDisplay.from_estimator(gbm_grid.best_estimator_, X_train, features=features, target=[4], ax=ax4)\n",
        "PartialDependenceDisplay.from_estimator(gbm_grid.best_estimator_, X_train, features=features, target=[5], ax=ax5);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUFQCMhl0oqH"
      },
      "source": [
        "Note that the probabilities shown are approximately in line with the balance between these classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "0ZzYLfHc0oqH"
      },
      "outputs": [],
      "source": [
        "(pd.Series(le.inverse_transform(y_train)).value_counts() / len(y_train)).loc[le.classes_]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "3y5H6T_00oqI"
      },
      "source": [
        "For further research into model explainability, other packages exist that go much deeper.\n",
        "\n",
        "[`lime` (Local Interpretable Model-agnostic Explanations)](https://github.com/marcotcr/lime) and [`shap` (SHapley Additive exPlanations)](https://shap.readthedocs.io/en/latest/) are two popular techniques used for explaining the predictions of machine learning models. They both aim to provide local interpretability, meaning they explain individual predictions of a model rather than its overall behavior. Both LIME and SHAP are powerful tools for understanding the behavior of complex machine learning models and gaining insights into the factors that drive individual predictions. They provide interpretability to models without compromising their predictive power, helping practitioners, users, and stakeholders trust and use AI systems more effectively and responsibly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "L8XMWaSX0oqI"
      },
      "source": [
        "## Concluding Tips\n",
        "\n",
        "1. Prioritize Feature Engineering above all else.\n",
        "2. Hyperparameter tuning is important but usually provides marginal improvements over feature engineering in traditional ML space.\n",
        "3. GBMs are a nice blend of power and ease-of-use. They should be your goto for most tabular data problems.\n",
        "4. Always cross-validate. Your training scores are lying to you.\n",
        "5. The baseline performance you have to beat is not zero, its the NULL model.\n",
        "6. Consider whether the default model performance metrics are appropriate for your specific task.\n",
        "7. Use Pipelines to encapsulate multistep models. This reduces the risk of cross-contamination during cross-validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsYHXeeSs-rf"
      },
      "source": [
        "## Bonus - AutoML\n",
        "\n",
        "Machine learning is fundamentally an exercise in experimentation.  AutoML gets rid of a lot of the tedium that goes into experimentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YspARMroTs6h",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from pycaret.classification import ClassificationExperiment\n",
        "\n",
        "s = ClassificationExperiment()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoML generally involves setting up the base problem. For this example, we set up a multiclass problem which `pycaret` autodetects from our response variable. It also handles the train/test splitting."
      ],
      "metadata": {
        "id": "eJy0jhzOQx_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "s.setup(\n",
        "    data.reset_index().drop('GRNAME', axis=1),\n",
        "    target='LOB',\n",
        "    session_id=42)"
      ],
      "metadata": {
        "id": "3wZuyzgbGDwk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calling the `compare_models` method will compare several models available, including those we reviewed above. It further compares across various performance metrics."
      ],
      "metadata": {
        "id": "p5tDd5mfRQQq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShrWUlCTUJYH",
        "tags": []
      },
      "outputs": [],
      "source": [
        "best = s.compare_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g87bRto1q8gC",
        "tags": []
      },
      "outputs": [],
      "source": [
        "print(best)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best model of the bunch can further be tuned."
      ],
      "metadata": {
        "id": "KNDjjW_bRjv1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "466uasfMMoiT",
        "tags": []
      },
      "outputs": [],
      "source": [
        "tuned_model = s.tune_model(best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAQWUXJE0oqI"
      },
      "outputs": [],
      "source": [
        "tuned_model"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6TLxw54aR0a1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}